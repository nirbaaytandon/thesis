\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Schmidhuber and Hochreiter(1997)]{lstmoriginal}
J{\"u}rgen Schmidhuber and Sepp Hochreiter.
\newblock Long short-term memory.
\newblock \emph{Neural Comput}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Graves and Schmidhuber(2005)]{lstmBiLSTM}
A.~Graves and J.~Schmidhuber.
\newblock Framewise phoneme classification with bidirectional lstm networks.
\newblock 4:\penalty0 2047--2052 vol. 4, 2005.
\newblock \doi{10.1109/IJCNN.2005.1556215}.

\bibitem[Tan et~al.(2015)Tan, Xiang, and Zhou]{lstmhaighextractive}
Ming Tan, Bing Xiang, and Bowen Zhou.
\newblock Lstm-based deep learning models for non-factoid answer selection.
\newblock \emph{CoRR}, abs/1511.04108, 2015.
\newblock URL \url{http://arxiv.org/abs/1511.04108}.

\bibitem[Wang and Jiang(2016)]{lstmhu2016question}
Shuohang Wang and Jing Jiang.
\newblock Machine comprehension using match-lstm and answer pointer.
\newblock \emph{CoRR}, abs/1608.07905, 2016.
\newblock URL \url{http://arxiv.org/abs/1608.07905}.

\bibitem[Gennaro et~al.(2020)Gennaro, Buonanno, Girolamo, Ospedale, and
  Palmieri]{lstmintent}
Giovanni~Di Gennaro, Amedeo Buonanno, Antonio~Di Girolamo, Armando Ospedale,
  and Francesco A.~N. Palmieri.
\newblock Intent classification in question-answering using {LSTM}
  architectures.
\newblock \emph{CoRR}, abs/2001.09330, 2020.
\newblock URL \url{https://arxiv.org/abs/2001.09330}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{atayl}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Feng et~al.(2015)Feng, Xiang, Glass, Wang, and Zhou]{lstmInsuranceQA}
Minwei Feng, Bing Xiang, Michael~R. Glass, Lidan Wang, and Bowen Zhou.
\newblock Applying deep learning to answer selection: {A} study and an open
  task.
\newblock \emph{CoRR}, abs/1508.01585, 2015.
\newblock URL \url{http://arxiv.org/abs/1508.01585}.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{dataset}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock \emph{CoRR}, abs/1806.03822, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.03822}.

\bibitem[Green~Jr et~al.(1961)Green~Jr, Wolf, Chomsky, and
  Laughery]{green1961baseball}
Bert~F Green~Jr, Alice~K Wolf, Carol Chomsky, and Kenneth Laughery.
\newblock Baseball: an automatic question-answerer.
\newblock In \emph{Papers presented at the May 9-11, 1961, western joint
  IRE-AIEE-ACM computer conference}, pages 219--224, 1961.

\bibitem[Woods and WA(1977)]{lunar}
William~A Woods and WOODS WA.
\newblock Lunar rocks in natural english: Explorations in natural language
  question answering.
\newblock 1977.

\bibitem[Team()]{hfTransformers}
The Hugging~Face Team.
\newblock Transformers.
\newblock URL \url{https://huggingface.co/transformers/}.
\newblock Accessed: 2021-04-16.

\bibitem[Rajpurkar(2021)]{squad}
P~Rajpurkar.
\newblock Squad2.0, 2021.
\newblock URL \url{https://rajpurkar.github.io/SQuAD-explorer/}.
\newblock Accessed: 2021-04-16.

\bibitem[Weissenborn et~al.(2017)Weissenborn, Wiese, and Seiffe]{fastQA}
Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
\newblock Fastqa: {A} simple and efficient neural architecture for question
  answering.
\newblock \emph{CoRR}, abs/1703.04816, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.04816}.

\bibitem[Ring(1993)]{lstmRing}
Mark~B Ring.
\newblock Learning sequential tasks by incrementally adding higher orders.
\newblock \emph{Advances in neural information processing systems}, pages
  115--115, 1993.

\bibitem[Hermann et~al.(2015)Hermann, Kocisk{\'{y}}, Grefenstette, Espeholt,
  Kay, Suleyman, and Blunsom]{bilstmherman}
Karl~Moritz Hermann, Tom{\'{a}}s Kocisk{\'{y}}, Edward Grefenstette, Lasse
  Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock \emph{CoRR}, abs/1506.03340, 2015.
\newblock URL \url{http://arxiv.org/abs/1506.03340}.

\bibitem[Brahma(2018)]{lstmSubilstm}
Siddhartha Brahma.
\newblock Suffix bidirectional long short-term memory.
\newblock \emph{CoRR}, abs/1805.07340, 2018.
\newblock URL \url{http://arxiv.org/abs/1805.07340}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{dataset1}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100, 000+ questions for machine comprehension of text.
\newblock \emph{CoRR}, abs/1606.05250, 2016.
\newblock URL \url{http://arxiv.org/abs/1606.05250}.

\bibitem[Wang and Jiang(2015)]{lstmMatch}
Shuohang Wang and Jing Jiang.
\newblock Learning natural language inference with {LSTM}.
\newblock \emph{CoRR}, abs/1512.08849, 2015.
\newblock URL \url{http://arxiv.org/abs/1512.08849}.

\bibitem[Vinyals et~al.(2015)Vinyals, Fortunato, and Jaitly]{lstmPointer}
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
\newblock Pointer networks.
\newblock \emph{arXiv preprint arXiv:1506.03134}, 2015.

\bibitem[Dietz et~al.(2017)Dietz, Verma, Radlinski, and Craswell]{trec}
Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell.
\newblock Trec complex answer retrieval overview.
\newblock In \emph{TREC}, 2017.

\bibitem[Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling,
  Monz, Pecina, Post, Saint-Amand, Soricut, Specia, and Tamchyna]{wmt}
Ond{\v{r}}ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp
  Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve
  Saint-Amand, Radu Soricut, Lucia Specia, and Ale{\v{s}} Tamchyna.
\newblock Findings of the 2014 workshop on statistical machine translation.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, pages 12--58, Baltimore, Maryland, USA, June 2014. Association
  for Computational Linguistics.
\newblock \doi{10.3115/v1/W14-3302}.
\newblock URL \url{https://www.aclweb.org/anthology/W14-3302}.

\bibitem[Zhou et~al.(2016)Zhou, Cao, Wang, Li, and Xu]{recurrent}
Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.
\newblock Deep recurrent models with fast-forward connections for neural
  machine translation.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  4:\penalty0 371--383, 2016.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and Marcinkiewicz]{wsj}
Mitchell Marcus, Beatrice Santorini, and Mary~Ann Marcinkiewicz.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock 1993.

\bibitem[Dai and Le(2015)]{dai}
Andrew~M Dai and Quoc~V Le.
\newblock Semi-supervised sequence learning.
\newblock \emph{arXiv preprint arXiv:1511.01432}, 2015.

\bibitem[Wang et~al.(2018)Wang, Yan, and Wu]{wang}
Wei Wang, Ming Yan, and Chen Wu.
\newblock Multi-granularity hierarchical attention fusion networks for reading
  comprehension and question answering.
\newblock \emph{arXiv preprint arXiv:1811.11934}, 2018.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{distil}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of {BERT:} smaller, faster, cheaper
  and lighter.
\newblock \emph{CoRR}, abs/1910.01108, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.01108}.

\bibitem[Corsair(2021)]{RAM}
Corsair.
\newblock Vengeance® lpx 8gb (1 x 8gb) ddr4 dram 2400mhz c14 memory kit -
  black, 2021.
\newblock URL
  \url{https://www.corsair.com/uk/en/Categories/Products/Memory/VENGEANCE-LPX/p/CMK8GX4M1A2400C14}.
\newblock Accessed: 2021-04-16.

\bibitem[Akbik et~al.(2018)Akbik, Blythe, and Vollgraf]{contextual}
Alan Akbik, Duncan Blythe, and Roland Vollgraf.
\newblock Contextual string embeddings for sequence labeling.
\newblock In \emph{Proceedings of the 27th international conference on
  computational linguistics}, pages 1638--1649, 2018.

\bibitem[Evga(2021)]{evga}
Evga.
\newblock Evga geforce rtx 2070 super ko gaming, 08g-p4-2072-kr, 8gb gddr6,
  dual fans, 2021.
\newblock URL
  \url{https://eu.evga.com/products/product.aspx?pn=08G-P4-2072-KR}.
\newblock Accessed: 2021-04-16.

\bibitem[Intel()]{intel}
Intel.
\newblock Intel® core™ i7-10700 processor (16m cache, up to 4.80 ghz) -
  product specifications.
\newblock URL
  \url{https://www.intel.co.uk/content/www/uk/en/products/sku/199316/intel-core-i710700-processor-16m-cache-up-to-4-80-ghz/specifications.html}.

\bibitem[Huang et~al.(2020)Huang, Xu, Liang, Mishra, and
  Xiang]{transformerBilstm}
Zhiheng Huang, Peng Xu, Davis Liang, Ajay Mishra, and Bing Xiang.
\newblock {TRANS-BLSTM:} transformer with bidirectional {LSTM} for language
  understanding.
\newblock \emph{CoRR}, abs/2003.07000, 2020.
\newblock URL \url{https://arxiv.org/abs/2003.07000}.

\bibitem[Sun et~al.(2018)Sun, Li, Qiu, and Liu]{unet}
Fu~Sun, Linyang Li, Xipeng Qiu, and Yang Liu.
\newblock U-net: Machine reading comprehension with unanswerable questions.
\newblock \emph{arXiv preprint arXiv:1810.06638}, 2018.

\end{thebibliography}
