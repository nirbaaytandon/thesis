
\documentclass[11pt]{article}


\usepackage{epsfig}
\usepackage{latexsym}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\begin{document}

\title{Research Proposal}

\author{Nirbhay P. Tandon\\968675\\Email: N.P.Tandon@2021.ljmu.ac.uk\\
Project Supervisor: Mr. Ankit Jha
}
\date{}
\maketitle


\begin{abstract}
Attention based Transformer architectures have become the norm of modern day Natural Language Processing. Google began this trend back in 2017 with their paper \textit{Attention Is All You Need}\cite{atayl}, by introducing the Transformer architecture that works solely on attention mechanisms. The purpose of our work will be to explore a new kind of Transformer architecture. Compare \& contrast its performance against the SQuAD 2.0 Dataset\cite{dataset} based other architectures such as BERT\cite{bert}, RoBERTa\cite{roberta}, ALBERT.
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction}\label{Introduction}

\section{Background \& Related Research}\label{Section-Frameworks}

\subsection{Background}
\subsection{Related Research}

\newpage
\section{Research Questions}\label{Section-Conclusions}
Highlighted below are some of the questions that we will answer through our research.

\
\newpage
\section{Aims \& Objectives}

Through our research we aim to establish the efficiency of our new Transformer architecture. We shall implement the existing models that are available via libraries such as HuggingFace, PyTorch \& Tensorflow, run the SQuAD 2.0\cite{dataset}, to obtain benchmark scores \& then compare the results with our proposed architecture. We hope to establish our proposed transformer architecture as a competent enough contender to be used within both industry \& academia.
\section{Research Methodology}
\section{Expected Outcomes}
\section{Requirements \& Resources}
\section{Research Plan}
\bibliographystyle{plain}
\bibliography{Research_Proposal}

\end{document}
