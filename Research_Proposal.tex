
\documentclass[11pt]{article}


\usepackage{epsfig}
\usepackage{latexsym}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\begin{document}

\title{Research Proposal}

\author{Nirbhay P. Tandon\\968675\\Email: ULPNTAND@ljmu.ac.uk\\
Project Supervisor: Mr. Ankit Jha
}
\date{}
\maketitle


\begin{abstract}
Attention based Transformer architectures have become the norm of modern day Natural Language Processing. Google began this trend back in 2017 with their paper \textit{Attention Is All You Need}\cite{atayl}, by introducing the Transformer architecture that works solely on attention mechanisms. The purpose of our work will be to explore a new kind of Transformer architecture. Compare \& contrast its performance against the SQuAD 2.0 Dataset\cite{dataset} based other architectures such as BERT\cite{bert}, RoBERTa\cite{roberta}, etc. Through our research we aim to produce a new Transformer Architecture that allows **Add additional details around how neural nets \& lstms compare to transformers. Fill in the final objective of the research proposal*
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction}\label{Introduction}

\section{Background \& Related Research}\label{Section-Frameworks}

\subsection{Background}
..is this just literature review now?
\subsection{Related Research}
Below is just papers that have been read and their summary. 

\begin{itemize}
	\item Transformers were produced as a result of a simplification process carried out in \cite{atayl}. 
\end{itemize}
\newpage
\section{Research Questions}\label{Section-Conclusions}
Highlighted below are some of the questions that we will answer through our research.

1. 

\
\newpage
\section{Aims \& Objectives}

Through our research we aim to establish the efficiency of our new Transformer architecture. We shall implement the existing models that are available via libraries such as HuggingFace\cite{hfTransformers}, PyTorch \& Tensorflow, run the SQuAD 2.0\cite{dataset}, to obtain benchmark scores \& then compare the results with our proposed architecture. We hope to establish our proposed transformer architecture as a competent enough contender to be used within both industry \& academia.
\section{Research Methodology}

To implement this research we shall break the project down into 5 phases. These are outlined below.
\subsection{Literature Review}
In this phase we will review the research that has been published already around the different kinds of architectures, shortlist some of the most widely used ones, compare their results using the SQuAD2.0 \cite{dataset} \& outline the pros and cons of each of these architectures. The rationale is to review \& understand as much of the research as possible so that we can avoid potential pitfalls, not duplicate our efforts by reinventing the wheel \& organize a better approach to perform our research.\textit{ **to add some pointers about how existing research has been done**}
\subsection{Research Benchmarks}
Here we shall focus on obtaining benchmark scores for the shortlisted architectures above using the dataset\cite{dataset}. The parameters used will be validation \& test set scores of the models. The training shall be carried out on each of the models for a 100(\textit{**TBD, 100 epochs per model for Squad will take over 100 hours of training, not sure if its worth it**}) epochs. We shall also look at the specificity/recall of these results to better understand if our work was done correctly or not.
\subsection{Architecture Creation}

In this stage we will use the various research methodologies that we have seen through our literature review \& implement those feature which we feel will yield the best results. This will be carried out by:
\begin{enumerate}
\item { Identifying and curating the best possible architectures}
\item {Highlighting the problems with those architectures}
\item { Implementing a rough architecture }
\item {Refining the architecture to quantify results}
\item { Producing a stage 1 MVP Model architecture}
\item { Attempting to generate an architecture similar to LSTMs, like nested Transformers}
\end{enumerate}
\subsection{Research Findings}

In this section we will enlist all our results \& findings. A detailed analysis will be provided against the existing architectures, their performance \& our proposed architecture.
\subsection{Conclusion}
...not sure what to put here. Ask Ankit for help on this. 
\section{Expected Outcomes}

We expect that the final architecture developed will be able to perform at par with the current state-of-the art architectures. Our objective is also to publish a paper on this architecture.
\section{Requirements \& Resources}
\section{Research Plan}
Ask Ankit what is the best approach to do this.
\bibliographystyle{plain}
\bibliography{Research_Proposal}

\end{document}
