\begin{thebibliography}{12}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{albert}
Z.~Lan, M.~Chen, S.~Goodman, K.~Gimpel, P.~Sharma, and R.~Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[Tan et~al.(2015)Tan, Xiang, and Zhou]{haighextractive}
M.~Tan, B.~Xiang, and B.~Zhou.
\newblock Lstm-based deep learning models for non-factoid answer selection.
\newblock \emph{CoRR}, abs/1511.04108, 2015.
\newblock URL \url{http://arxiv.org/abs/1511.04108}.

\bibitem[Dai and Le(2015)]{dai}
A.~M. Dai and Q.~V. Le.
\newblock Semi-supervised sequence learning.
\newblock \emph{arXiv preprint arXiv:1511.01432}, 2015.

\bibitem[Wang et~al.(2018)Wang, Yan, and Wu]{wang}
W.~Wang, M.~Yan, and C.~Wu.
\newblock Multi-granularity hierarchical attention fusion networks for reading
  comprehension and question answering.
\newblock \emph{arXiv preprint arXiv:1811.11934}, 2018.

\bibitem[Zhou et~al.(2016)Zhou, Cao, Wang, Li, and Xu]{recurrent}
J.~Zhou, Y.~Cao, X.~Wang, P.~Li, and W.~Xu.
\newblock Deep recurrent models with fast-forward connections for neural
  machine translation.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  4:\penalty0 371--383, 2016.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and Marcinkiewicz]{wsj}
M.~Marcus, B.~Santorini, and M.~A. Marcinkiewicz.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock 1993.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{atayl}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{dataset}
P.~Rajpurkar, R.~Jia, and P.~Liang.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock \emph{CoRR}, abs/1806.03822, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.03822}.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Bengio, Frasconi, Schmidhuber,
  et~al.]{lstm}
S.~Hochreiter, Y.~Bengio, P.~Frasconi, J.~Schmidhuber, et~al.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies, 2001.

\bibitem[Weissenborn et~al.(2017)Weissenborn, Wiese, and Seiffe]{fastQA}
D.~Weissenborn, G.~Wiese, and L.~Seiffe.
\newblock Fastqa: {A} simple and efficient neural architecture for question
  answering.
\newblock \emph{CoRR}, abs/1703.04816, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.04816}.

\end{thebibliography}
