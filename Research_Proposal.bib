@article{dataset,
	author    = {Pranav Rajpurkar and
	Robin Jia and
	Percy Liang},
	title     = {Know What You Don't Know: Unanswerable Questions for SQuAD},
	journal   = {CoRR},
	volume    = {abs/1806.03822},
	year      = {2018},
	url       = {http://arxiv.org/abs/1806.03822},
	archivePrefix = {arXiv},
	eprint    = {1806.03822},
	timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1806-03822.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{recurrent,
	title={Deep recurrent models with fast-forward connections for neural machine translation},
	author={Zhou, Jie and Cao, Ying and Wang, Xuguang and Li, Peng and Xu, Wei},
	journal={Transactions of the Association for Computational Linguistics},
	volume={4},
	pages={371--383},
	year={2016},
	publisher={MIT Press}
}
@inproceedings{zhang,
	title = "Position-aware Attention and Supervised Data Improve Slot Filling",
	author = "Zhang, Yuhao  and
	Zhong, Victor  and
	Chen, Danqi  and
	Angeli, Gabor  and
	Manning, Christopher D.",
	booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2017",
	address = "Copenhagen, Denmark",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D17-1004",
	doi = "10.18653/v1/D17-1004",
	pages = "35--45",
	abstract = "Organized relational knowledge in the form of {``}knowledge graphs{''} is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2{\%} to 26.7{\%}.",
}
@article{schmid,
	title={Deep learning in neural networks: An overview},
	author={Schmidhuber, J{\"u}rgen},
	journal={Neural networks},
	volume={61},
	pages={85--117},
	year={2015},
	publisher={Elsevier}
}
@article{levy,
	title={Zero-shot relation extraction via reading comprehension},
	author={Levy, Omer and Seo, Minjoon and Choi, Eunsol and Zettlemoyer, Luke},
	journal={arXiv preprint arXiv:1706.04115},
	year={2017}
}
@inproceedings{contextual,
	title={Contextual string embeddings for sequence labeling},
	author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
	booktitle={Proceedings of the 27th international conference on computational linguistics},
	pages={1638--1649},
	year={2018}
}
@misc{lstm,
	title={Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
	author={Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen and others},
	year={2001},
	publisher={A field guide to dynamical recurrent neural networks. IEEE Press}
}
@article{wsj,
	title={Building a large annotated corpus of English: The Penn Treebank},
	author={Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
	year={1993}
}
@article{atayl,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	journal={arXiv preprint arXiv:1706.03762},
	year={2017}
}
@article{bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}
@article{albert,
	title={Albert: A lite bert for self-supervised learning of language representations},
	author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	journal={arXiv preprint arXiv:1909.11942},
	year={2019}
}
@article{fastQA,
	author    = {Dirk Weissenborn and
	Georg Wiese and
	Laura Seiffe},
	title     = {FastQA: {A} Simple and Efficient Neural Architecture for Question
	Answering},
	journal   = {CoRR},
	volume    = {abs/1703.04816},
	year      = {2017},
	url       = {http://arxiv.org/abs/1703.04816},
	archivePrefix = {arXiv},
	eprint    = {1703.04816},
	timestamp = {Mon, 13 Aug 2018 16:46:13 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/WeissenbornWS17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}
@article{originallstm,
	title={Long short-term memory},
	author={Schmidhuber, J{\"u}rgen and Hochreiter, Sepp},
	journal={Neural Comput},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997}
}
@article{distil,
	author    = {Victor Sanh and
	Lysandre Debut and
	Julien Chaumond and
	Thomas Wolf},
	title     = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
	and lighter},
	journal   = {CoRR},
	volume    = {abs/1910.01108},
	year      = {2019},
	url       = {http://arxiv.org/abs/1910.01108},
	archivePrefix = {arXiv},
	eprint    = {1910.01108},
	timestamp = {Tue, 02 Jun 2020 12:48:59 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{evga,
	author={Evga},
	title={EVGA GeForce RTX 2070 SUPER KO GAMING, 08G-P4-2072-KR, 8GB GDDR6, Dual Fans}, 
	url={https://eu.evga.com/products/product.aspx?pn=08G-P4-2072-KR}, 
	note = {Accessed: 2021-04-16}
} 
 @misc{squad, title={SQuAD2.0}, url={https://rajpurkar.github.io/SQuAD-explorer/}, journal={The Stanford Question Answering Dataset}, author={Rajpurkar, P}} 
@misc{RAM,
	author={Corsair},
	url={https://www.corsair.com/uk/en/Categories/Products/Memory/VENGEANCE-LPX/p/CMK8GX4M1A2400C14}, 
	title={VENGEANCE® LPX 8GB (1 x 8GB) DDR4 DRAM 2400MHz C14 Memory Kit - Black},
	note = {Accessed: 2021-04-16}
} 

@misc{intel, 
		author={Intel},
		title={Intel® Core™ i7-10700 Processor (16M Cache, up to 4.80 GHz) - Product Specifications}, url={https://www.intel.co.uk/content/www/uk/en/products/sku/199316/intel-core-i710700-processor-16m-cache-up-to-4-80-ghz/specifications.html}, journal={Intel}} 

@misc{hfTransformers, 
	author={The Hugging Face Team},
	title={Transformers},
	url={https://huggingface.co/transformers/}, 
	journal={Transformers - transformers 4.5.0.dev0 documentation},
	note = {Accessed: 2021-04-16}} 